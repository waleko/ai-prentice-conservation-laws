{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we try to estimate the dimensionality of the dynamic trajectory subspace.\n",
    "\n",
    "### Helpful articles\n",
    "- [Supplementary material from AI Poincare](https://journals.aps.org/prl/supplemental/10.1103/PhysRevLett.126.180604/poincare_supplemental_materialv2.pdf)\n",
    "- [Interpretable conservation law estimation by deriving the symmetries of dynamics from trained deep neural networks](https://journals.aps.org/pre/pdf/10.1103/PhysRevE.103.033303) cited in AI Poincare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from experiments import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"Autoencoders Trajectory (average, cluster)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, intermediate_dim: int):\n",
    "        super(AE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, intermediate_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(intermediate_dim, intermediate_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(intermediate_dim, intermediate_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(intermediate_dim, intermediate_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(intermediate_dim, intermediate_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(intermediate_dim, intermediate_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(intermediate_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, intermediate_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(intermediate_dim, intermediate_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(intermediate_dim, intermediate_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(intermediate_dim, intermediate_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(intermediate_dim, intermediate_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(intermediate_dim, intermediate_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(intermediate_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_point(dim=3):\n",
    "    x = np.random.rand(dim)\n",
    "    x[0] *= np.random.choice([-1, 1])\n",
    "    x[1] *= np.random.choice([-1, 1])\n",
    "    return x / np.linalg.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\" # cpu is faster ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataloader, valid_dataloader, input_dim: int, experiment_name: str, hidden_layer_dim: int, epochs=1000, intermediate_dim=20, batch_size=10):\n",
    "    model = AE(input_dim, hidden_layer_dim, intermediate_dim).to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    counter = 0\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "\n",
    "        model.train()\n",
    "        for batch_pts in train_dataloader:\n",
    "            inp = batch_pts.float().to(device)\n",
    "            output = model(inp)\n",
    "            loss = criterion(output, inp)\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        model.eval()\n",
    "        for batch_pts in valid_dataloader:\n",
    "            inp = batch_pts.float().to(device)\n",
    "            output = model(inp)\n",
    "            loss = criterion(output, inp)\n",
    "            valid_losses.append(loss.item())\n",
    "        \n",
    "        train_loss = np.sqrt(np.average(train_losses))\n",
    "        valid_loss = np.sqrt(np.average(valid_losses))\n",
    "\n",
    "        wandb.log({f\"{experiment_name}_{hidden_layer_dim}_train_loss\": train_loss})\n",
    "        wandb.log({f\"{experiment_name}_{hidden_layer_dim}_val_loss\": valid_loss})\n",
    "        \n",
    "        if valid_loss < decision_threshold / 2:\n",
    "            counter += 1\n",
    "        else:\n",
    "            counter = 0\n",
    "        if counter > 50:\n",
    "            wandb.alert(title=\"Early stopping\", text=f\"Early stopping for {experiment_name}{hidden_layer_dim} on epoch #{epoch}/{epochs}\")\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_train(traj, experiment_name, max_hidden_size, epochs=1000, intermediate_dim=20, batch_size=32, start_embedding_size=1):\n",
    "    traj_scaled = MaxAbsScaler().fit_transform(traj) # scale\n",
    "\n",
    "    traj_train, traj_val, traj_test = random_split(traj_scaled, [0.8, 0.1, 0.1])\n",
    "\n",
    "    train_dataloader = DataLoader(traj_train, batch_size=batch_size, shuffle=True)\n",
    "    valid_dataloader = DataLoader(traj_val, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(traj_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    models = []\n",
    "    model_losses = []\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    \n",
    "    for hidden_layer_size in tqdm(range(start_embedding_size, max_hidden_size + 1)):\n",
    "        model = train_model(train_dataloader, valid_dataloader, traj.shape[1], experiment_name, hidden_layer_size, intermediate_dim=intermediate_dim, batch_size=batch_size, epochs=epochs)\n",
    "        models.append(model)\n",
    "        # test loss\n",
    "        model.eval()\n",
    "        test_losses = []\n",
    "        for batch_pts in test_dataloader:\n",
    "            inp = batch_pts.float().to(device)\n",
    "            output = model(inp)\n",
    "            loss = criterion(output, inp)\n",
    "            test_losses.append(loss.item())\n",
    "        test_loss = np.sqrt(np.average(test_losses))\n",
    "        wandb.log({f\"test_loss_{experiment_name}_{hidden_layer_size}\": test_loss})\n",
    "        model_losses.append(test_loss)\n",
    "    return models, model_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_threshold = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses_errorbars(experiment_name, n_eff, model_losses, max_hidden_size, start_embedding_size=1):\n",
    "    loss_mean = np.mean(model_losses.T, axis=1)\n",
    "    loss_std = np.std(model_losses.T, axis=1)\n",
    "\n",
    "    hidden_sizes = range(start_embedding_size, max_hidden_size + 1)\n",
    "    colors = [1 if x < n_eff else 0 for x in hidden_sizes]\n",
    "\n",
    "    plt.errorbar(hidden_sizes, loss_mean, yerr=loss_std, fmt='none', ecolor=colors)\n",
    "    plt.scatter(hidden_sizes, loss_mean, c=colors)\n",
    "    plt.axhline(y=decision_threshold, color='blue', linestyle='--')\n",
    "    plt.xticks(hidden_sizes)\n",
    "    title = f\"{experiment_name} n_eff={n_eff}\"\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"hidden layer size\")\n",
    "    plt.ylabel(\"r.m.s. loss after auto-encoder\")\n",
    "    plt.savefig(f\"plot_{experiment_name}_2e-2\")\n",
    "    wandb.log({title: wandb.Image(f\"plot_{experiment_name}_2e-2.png\")})\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_n_eff(exp: PhysExperiment, epochs=5000, intermediate_dim=20, batch_size=10):\n",
    "    traj = exp.single_trajectory(42)\n",
    "    n_eff = exp.n_eff\n",
    "    model = train_model(traj, exp.experiment_name, n_eff, epochs, intermediate_dim, batch_size)\n",
    "    test_traj = torch.Tensor(traj).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = model.encoder(test_traj).detach().cpu().numpy()\n",
    "        transformed = model(test_traj).detach().cpu().numpy()\n",
    "\n",
    "        all_trajs = np.concatenate((traj, transformed))\n",
    "        color = np.concatenate((np.zeros(shape=(traj.shape[0], 1)), np.ones(shape=(transformed.shape[0], 1))))\n",
    "        exported = np.append(all_trajs, color, axis=1)\n",
    "        table = wandb.Table(columns=exp.column_names + [\"transformed\"], data=exported)\n",
    "        wandb.log({f\"{exp.experiment_name} before/after\": table})\n",
    "    if n_eff == 1:\n",
    "        # coloring\n",
    "        traj_with_color = np.append(traj, embedding, axis=1)\n",
    "        wandb.log({f\"{exp.experiment_name} coloring for n_eff=1 embedding\": wandb.Table(exp.column_names + [\"color\"], data=traj_with_color)})\n",
    "    elif n_eff == 2:\n",
    "        # 2d embedding\n",
    "        wandb.log({f\"{exp.experiment_name} 2d n_eff embedding\": wandb.Table([\"projection1\", \"projection2\"], embedding)})\n",
    "    elif n_eff == 3:\n",
    "        # 3d embedding\n",
    "        wandb.log({f\"{exp.experiment_name} 3d n_eff embedding\": wandb.Object3D(embedding)})\n",
    "    else:\n",
    "        wandb.alert(f\"no visual representation for n_eff={n_eff} with experiment {exp.experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_device = device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# for exp in [KdV]:\n",
    "#     traj = exp.single_trajectory(42)\n",
    "#     max_hidden_size = exp.n_eff + 2\n",
    "#     min_hidden_size = exp.n_eff - 3\n",
    "    \n",
    "#     models, epochs = full_train(traj, exp.experiment_name, max_hidden_size, epochs=5000, batch_size=20, start_embedding_size=min_hidden_size, intermediate_dim=500)\n",
    "#     plot_losses(exp.experiment_name, exp.n_eff, epochs, max_hidden_size, start_embedding_size=min_hidden_size)\n",
    "# device = old_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp in tqdm(common_experiments, position=0):\n",
    "    max_hidden_size = exp.n_eff + 2\n",
    "    losses_2d = []\n",
    "    for _ in tqdm(range(3), position=1, leave=False):\n",
    "        traj = exp.single_trajectory()\n",
    "        models, epochs = full_train(traj, exp.experiment_name, max_hidden_size, epochs=5000, batch_size=20)\n",
    "        losses_2d.append(epochs)\n",
    "    losses_2d = np.array(losses_2d)\n",
    "    print(losses_2d)\n",
    "    # wandb.log({exp.experiment_name: losses_2d})\n",
    "    # plot_losses_errorbars(exp.experiment_name, exp.n_eff, losses_2d, max_hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(project=\"AE embedding & Turing\")\n",
    "# ll = [\n",
    "#     # Pendulum,\n",
    "#     # HarmonicOscillator,\n",
    "#     # DoublePendulum,\n",
    "#     # DoublePendulumSmallEnergy,\n",
    "#     DoublePendulumLargeEnergy,\n",
    "#     # CoupledOscillator,\n",
    "#     # KeplerProblem,\n",
    "#     # Sphere5\n",
    "# ]\n",
    "# device = \"cpu\"\n",
    "# for exp in ll:\n",
    "#     train_n_eff(exp, epochs=5000, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\"\n",
    "# for exp in [Turing]:\n",
    "#     traj = exp.single_trajectory(42)\n",
    "#     models, epochs = full_train(traj, exp.experiment_name, 103, epochs=5000, start_embedding_size=97, intermediate_dim=200)\n",
    "#     plot_losses(exp.experiment_name, exp.n_eff, epochs, 103, start_embedding_size=97)\n",
    "# wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10) \n[GCC 10.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
